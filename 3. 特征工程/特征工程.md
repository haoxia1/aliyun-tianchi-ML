# Note

[toc]

## 三、特征工程

### 1. 特征工程的重要性和处理

> 数据和特征决定了机器学习的上限，而模型和算法知识逼近这个上限而已
> 具体而言，特征越好、灵活性越强、构建的模型越简单、性能越出色
特征工程的流程：去掉无用特征->去掉冗余特征(eg 共线特征)->生成新特征->特征转换(数值化、类别转换、归一化)->特征处理

### 2. 数据预处理和特征处理

#### 2.1 数据预处理

1. 数据采集
2. 数据清洗（取出脏数据，比如某些商品的刷单数据）
   ![图 1](/images/9be5773af82950c7518feb098fe668c0a4223784d40d5c1d01b61ca531126376.png)  
    比如规定范围之外的样本值。
3. 数据采样
   1. 当正负样本不均衡时，可采取随机采样或分层采样。使正负样本均匀。
   2. 当正样本>负样本，且量都特别大，采用下采样(downsampling)(缩小图像)
   3. 当正样本<负样本，且量不大，上采样(oversampling)(放大图像)

#### 2.2 特征处理

- **==标准化==**$(Standardization)$
  - 将数据变换为均值为0，标准差为1的分布切记，并非一定是正态的；
  $$
  x' = \frac{x-\overline{X}}{S}
  $$

  ```python
  from sklearn.preprocessing import StandardScaler
  StandardScaler().fit_transform(iris.data)
  ```

- **区间缩放法**
  - 常见的是利用最值进行缩放到[0,1]
$$
x' = \frac{x-x_{min}}{x_{max}-x_{min}}
$$

```python
from sklearn.preprocessing import MinMaxScaler
MinMaxScaler().fit_transform(iris.data)
```

- **==归一化==**$(Normalization)$
  - 将样本的特征值转换到同一量纲/区间下，把数据映射到[0,1] 或 [a,b] 区间内，**由于其仅由变量的极值决定，因此区间缩放法是归一化的一种。**
  - **归一化的好处：加快梯度下降求解最优解的速度；**
  - **归一区间会改变数据的原始距离、分布和信息，但标准化一般不会。**
  - 规则为L2的归一化公式如下
$$
x' = \frac{x}{\sqrt{\sum_j^m x_j^2}}
$$

```python
from sklearn.preprocessing import Normalizer
Normalizer().fit_transform(iris.data)
```

- 归一化与标准化的使用场景：
  - 如果对**输出结果范围有要求**，则用归一化
  - 如果**数据较为稳定，不存在极端最值**，则用归一化（因其取决于变量的极值）
  - 如果**数据存在异常值或较多噪声**，则用标准化，如此可通过中心化间接避免异常值和极端值的影响
  - SVM、KNN、PCA等模型必须进行归一化或标准化

- **定量特征二值化**
$$
x'=\left\{
\begin{aligned}
1,x>threshold; \\
0,x\leq threshold; \\
\end{aligned}
\right.
$$

```python
from sklearn.preprocessing import Binarizer
Binarizer().fit_transform(iris.data)
```

- **定性特征哑编码**
  - 将多分类变量转换为哑变量（Dummy Variable）
  - eg 男0女1

- **缺失值处理**
  - pandas将缺失值表示为NaN
`SimpleImputer().fit_transform()`

- **数据转换**
  - 多项式转换
  - 对数转换

![图 2](/images/beb71126dfe5ec64ecd3dde6ca79a67188c84b68e2620068de070ac49de81e4d.png)  

### 3. 特征降维

![图 3](/images/cfb784593103ce7b45f6464884a512e7aee30ae8fbecaa9ee89495a6b65203bf.png)  
  特征降维的常用方法为**特征选择**和**线性降维**

#### 3.1 特征选择

简单粗暴，映射函数直接将不重要的特征删除，但会丢失特征信息。
![图 1](/images/10df1a88be1d09a5b876bbd29f31bad295b1ed52615d868c531bc1f63d3361b9.png)  

- **特征选择方法：**
  - 过滤法Filter
    - VarianceThreshold
 ![图 2](/images/7a60ee4e6bae06e2fc0709f83ac2cc66b54b93b511dd8ad80b6bdc00dbb8a4d9.png)  
![图 3](/images/630ff60287d06205c04c4c1278c5b94892e4ea29ccb689ed46148d602600f7c7.png)  
![图 4](/images/20a47f6a24bd95156861ff91b2eba8dabbf4e7fb6ce2df1ff1fcc45eada9a069.png)  

  - 包装法Wrapper
  - 嵌入法Embedded

![图 5](/images/4c18d3427eca062c7bbda7d3b775335e874fe2aad6d32c89d19adc10b0c60be3.png)  

#### 3.2 线性降维

- **主成分分析法PCA**（无监督）
  - 通过某种线性投影，将高维数据映射到低维空间，并期望在所投影的维度上数据的**方差最大**，以此达到使用较少的数据维度来保留较多的原数据点特性的效果。

```python
from sklearn.decomposition import PCA
PCA(n_components=2).fit_transform(iris.data) #n_components为主成分的数目
```

- **线性判别分析法LDA**（有监督）
  - 与PCA尽可能多地保留数据信息不同，LDA的目标是使降维后的数据点尽可能地容易被区分。“**投影后类内的方差尽可能小，类间方差尽可能大**”

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
LDA(n_components=2).fit_transform(iris.data) #n_components为降维后的维数
```

### 4. 赛题特征选择

1. 异常值分析`plt.boxplot()`
2. 最大值和最小值的归一化`MinMaxScaler().fit_transform()`
3. 查看数据分布`sns.kdeplot()`
4. 特征相关性`df.corr()`
5. 特征降维（筛选出corr>0.1的特征变量）
6. 多重共线性分析
7. PCA处理
