# Note

[toc]

## 三、特征工程

### 1. 特征工程的重要性和处理

> 数据和特征决定了机器学习的上限，而模型和算法知识逼近这个上限而已
> 具体而言，特征越好、灵活性越强、构建的模型越简单、性能越出色
特征工程的流程：去掉无用特征->去掉冗余特征(eg 共线特征)->生成新特征->特征转换(数值化、类别转换、归一化)->特征处理

### 2. 数据预处理和特征处理

#### 2.1 数据预处理

1. 数据采集
2. 数据清洗（取出脏数据，比如某些商品的刷单数据）
   ![图 1](/images/9be5773af82950c7518feb098fe668c0a4223784d40d5c1d01b61ca531126376.png)  
    比如规定范围之外的样本值。
3. 数据采样
   1. 当正负样本不均衡时，可采取随机采样或分层采样。使正负样本均匀。
   2. 当正样本>负样本，且量都特别大，采用下采样(downsampling)(缩小图像)
   3. 当正样本<负样本，且量不大，上采样(oversampling)(放大图像)

#### 2.2 特征处理

- **==标准化==**$(Standardization)$
  - 将数据变换为均值为0，标准差为1的分布切记，并非一定是正态的；
  $$
  x' = \frac{x-\overline{X}}{S}
  $$

  ```python
  from sklearn.preprocessing import StandardScaler
  StandardScaler().fit_transform(iris.data)
  ```

- **区间缩放法**
  - 常见的是利用最值进行缩放到[0,1]
$$
x' = \frac{x-x_{min}}{x_{max}-x_{min}}
$$

```python
from sklearn.preprocessing import MinMaxScaler
MinMaxScaler().fit_transform(iris.data)
```

- **==归一化==**$(Normalization)$
  - 将样本的特征值转换到同一量纲/区间下，把数据映射到[0,1] 或 [a,b] 区间内，**由于其仅由变量的极值决定，因此区间缩放法是归一化的一种。**
  - **归一化的好处：加快梯度下降求解最优解的速度；**
  - **归一区间会改变数据的原始距离、分布和信息，但标准化一般不会。**
  - 规则为L2的归一化公式如下
$$
x' = \frac{x}{\sqrt{\sum_j^m x_j^2}}
$$

```python
from sklearn.preprocessing import Normalizer
Normalizer().fit_transform(iris.data)
```

- 归一化与标准化的使用场景：
  - 如果对**输出结果范围有要求**，则用归一化
  - 如果**数据较为稳定，不存在极端最值**，则用归一化（因其取决于变量的极值）
  - 如果**数据存在异常值或较多噪声**，则用标准化，如此可通过中心化间接避免异常值和极端值的影响
  - SVM、KNN、PCA等模型必须进行归一化或标准化

- **定量特征二值化**
$$
x'=\left\{
\begin{aligned}
1,x>threshold; \\
0,x\leq threshold; \\
\end{aligned}
\right.
$$

```python
from sklearn.preprocessing import Binarizer
Binarizer().fit_transform(iris.data)
```

- **定性特征哑编码**
  - 将多分类变量转换为哑变量（Dummy Variable）
  - eg 男0女1

- **缺失值处理**
  - pandas将缺失值表示为NaN
`SimpleImputer().fit_transform()`

- **数据转换**
  - 多项式转换
  - 对数转换

![图 2](/images/beb71126dfe5ec64ecd3dde6ca79a67188c84b68e2620068de070ac49de81e4d.png)  

### 3. 特征降维

![图 3](/images/cfb784593103ce7b45f6464884a512e7aee30ae8fbecaa9ee89495a6b65203bf.png)  
  特征降维的常用方法为**特征选择**和**线性降维**

#### 3.1 特征选择

简单粗暴，映射函数直接将不重要的特征删除，但会丢失特征信息。
![图 1](/images/10df1a88be1d09a5b876bbd29f31bad295b1ed52615d868c531bc1f63d3361b9.png)  

- **特征选择方法：**
  - 过滤法Filter
    - VarianceThreshold
 ![图 2](/images/7a60ee4e6bae06e2fc0709f83ac2cc66b54b93b511dd8ad80b6bdc00dbb8a4d9.png)  
![图 3](/images/630ff60287d06205c04c4c1278c5b94892e4ea29ccb689ed46148d602600f7c7.png)  
![图 4](/images/20a47f6a24bd95156861ff91b2eba8dabbf4e7fb6ce2df1ff1fcc45eada9a069.png)  

  - 包装法Wrapper
  - 嵌入法Embedded

![图 5](/images/4c18d3427eca062c7bbda7d3b775335e874fe2aad6d32c89d19adc10b0c60be3.png)  

#### 3.2 线性降维

- **主成分分析法PCA**（无监督）
  - 通过某种线性投影，将高维数据映射到低维空间，并期望在所投影的维度上数据的**方差最大**，以此达到使用较少的数据维度来保留较多的原数据点特性的效果。

```python
from sklearn.decomposition import PCA
PCA(n_components=2).fit_transform(iris.data) #n_components为主成分的数目
```

- **线性判别分析法LDA**（有监督）
  - 与PCA尽可能多地保留数据信息不同，LDA的目标是使降维后的数据点尽可能地容易被区分。“**投影后类内的方差尽可能小，类间方差尽可能大**”

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
LDA(n_components=2).fit_transform(iris.data) #n_components为降维后的维数
```

### 4. 赛题特征选择

1. 异常值分析`plt.boxplot()`
2. 最大值和最小值的归一化`MinMaxScaler().fit_transform()`
3. 查看数据分布`sns.kdeplot()`
4. 特征相关性`df.corr()`
5. 特征降维（筛选出corr>0.1的特征变量）
6. 多重共线性分析
7. PCA处理

## 四. 模型训练

### 1. 回归及相关模型

#### 1.1 回归模型训练和预测步骤

1. 导入工具库
2. 数据预处理：include 导入数据集、处理数据等操作，具体为缺失值处理、连续特征归一化、类别特征转换等。
3. 训练模型：选择合适的model，利用训练集对model训练。
4. 预测结果：将待预测数据集输入到训练好的model。

#### 1.2 线性回归模型

Y与X呈线性相关
损失函数：$MSE$

- 一元线性回归模型
  - 单一特征自变量X估计y
  - 通过最小化代价函数得到
- 多元线性回归模型
  - 多个特征自变量X估计y

- 优点：模型简单、部署方便；训练快
- 缺点：精度低，特征存在一定的共线性问题
- 技巧：特征需归一化，建议进行一定的特征选择，尽量避免高度相关的特征同时存在。

#### 1.3 K近邻回归模型

KNN算法可用于分类和回归。回归中：找k个最近邻居，将这些邻居的某个属性的平均值赋给该样本，就找到了该样本对应属性的值。
损失函数：$MSE$

- 优点：模型简单、可快速处理数据量小的情况
- 缺点：计算量大，不适合数据量大的情况；需要调参；
- 技巧：特征需归一化，重要的特征需适当加一定比例的权重

#### 1.4 决策树回归模型

根据一定准则，将一个空间划分为若干个子空间，然后利用子空间内所有点的信息表示这个子空间的值；
如何回归预测? 利用这些**划分区域的均值或中位数**代表这个区域代表这个区域的预测值
损失函数：

$$
L(D) = \sum_{i=1}^k(y_i - \overline{y_1})^2 + \sum_{i=k+1}^N(y_i - \overline{y_2})^2
$$

遍历所有点求出L(D),找到最优分割点D；然后在D左和D右继续上述步骤，直到满足停止条件。

#### 1.5 集成学习回归模型

1. **随机森林回归模型**
   1. 通过集成学习的思想将多棵树集成的一种算法，基本单元是决策树；其本质属于机器学习的一个分支——集成学习$Ensemble\ Learning$
   2. 回归中，随机森林输出所有决策树输出的平均值
   3. 优点：
      1. 精度较高
      2. 能有效运行在大数据集上；
      3. 能够处理具有高维特征的输入样本，无需降维；
      4. 能评估各特征在分类问题上的重要性；等
   4. 缺点：结果不容易解释
   5. 技巧：参数调节，提高精度
2. **LightGBM回归模型**
   1. $Light Gradient Boosting Machine$是Microsoft开发的一个$GBDT(Gradient Boosting Decision Tree)$算法框架，具有更快的训练速度、更低的内存消耗、更好的准确率、分布式支持、可以快速处理海量数据等特征；
   2. 优点：精度高
   3. 缺点：慢，模型复杂
   4. 技巧：有效的验证集防止过拟合；参数搜索

#### 模型预测改善流程

1. 原始的train_data 分割为 train和target，再split为train和test
   1. LinearRegression_MSE:           0.116
   2. LinearRegression_R2_score:      0.889
   3. KNeighborsRegressor_MSE:        0.218
   4. KNeighborsRegressor_R2_score:   0.790
   5. DecisionTreeRegressor_MSE:      0.238
   6. DecisionTreeRegressor_R2_score: 0.771
   7. RandomForestRegressor_MSE:      0.132
   8. RandomForestRegressor_R2_score: 0.873
   9. lightGbm_MSE:                   0.114
   10. lightGbm_R2_score:             0.891
2. 将特征V9的两个异常值去掉(**MSE基本在下降，同时准确率基本在上升**)
   1. LinearRegression_MSE:           0.102
   2. LinearRegression_R2_score:      0.908
   3. KNeighborsRegressor_MSE:        0.214
   4. KNeighborsRegressor_R2_score:   0.806
   5. DecisionTreeRegressor_MSE:      0.263
   6. DecisionTreeRegressor_R2_score: 0.761
   7. RandomForestRegressor_MSE:      0.120
   8. RandomForestRegressor_R2_score: 0.891
   9. lightGbm_MSE:                   0.101
   10. lightGbm_R2_score:             0.908
3. 将特征**归一化**之后$min_max_scaler$（**为何模型归一化后MSE上升，准确率下降？**）
   1. LinearRegression_MSE:           0.241
   2. LinearRegression_R2_score:      0.765
   3. KNeighborsRegressor_MSE:        0.268
   4. KNeighborsRegressor_R2_score:   0.739
   5. DecisionTreeRegressor_MSE:      0.530
   6. DecisionTreeRegressor_R2_score: 0.483
   7. RandomForestRegressor_MSE:      0.228
   8. RandomForestRegressor_R2_score: 0.777
   9. lightGbm_MSE:                   0.201
   10. lightGbm_R2_score:             0.804
4. PCA处理选择16个特征(**为啥又tm降低了**)
   1. LinearRegression_MSE:           0.272
   2. LinearRegression_R2_score:      0.735
   3. KNeighborsRegressor_MSE:        0.268
   4. KNeighborsRegressor_R2_score:   0.739
   5. DecisionTreeRegressor_MSE:      0.588
   6. DecisionTreeRegressor_R2_score: 0.427
   7. RandomForestRegressor_MSE:      0.249
   8. RandomForestRegressor_R2_score: 0.758
   9. lightGbm_MSE:                   0.244
   10. lightGbm_R2_score:             0.762

> PCA降维的目的是防止维度灾难，PCA是为了挑选出权重高的特征
